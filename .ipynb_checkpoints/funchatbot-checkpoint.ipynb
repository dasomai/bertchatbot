{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72732557",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Using cached transformers-4.25.1-py3-none-any.whl (5.8 MB)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\daso\\anaconda3\\envs\\funchatbot\\lib\\site-packages (from transformers) (22.0)\n",
      "Collecting pyyaml>=5.1\n",
      "  Using cached PyYAML-6.0-cp310-cp310-win_amd64.whl (151 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.10.0\n",
      "  Using cached huggingface_hub-0.11.1-py3-none-any.whl (182 kB)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
      "  Using cached tokenizers-0.13.2-cp310-cp310-win_amd64.whl (3.3 MB)\n",
      "Collecting numpy>=1.17\n",
      "  Using cached numpy-1.24.1-cp310-cp310-win_amd64.whl (14.8 MB)\n",
      "Collecting tqdm>=4.27\n",
      "  Using cached tqdm-4.64.1-py2.py3-none-any.whl (78 kB)\n",
      "Collecting requests\n",
      "  Using cached requests-2.28.1-py3-none-any.whl (62 kB)\n",
      "Collecting filelock\n",
      "  Using cached filelock-3.9.0-py3-none-any.whl (9.7 kB)\n",
      "Collecting regex!=2019.12.17\n",
      "  Using cached regex-2022.10.31-cp310-cp310-win_amd64.whl (267 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\daso\\anaconda3\\envs\\funchatbot\\lib\\site-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.4.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\daso\\anaconda3\\envs\\funchatbot\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\daso\\anaconda3\\envs\\funchatbot\\lib\\site-packages (from requests->transformers) (2022.12.7)\n",
      "Collecting urllib3<1.27,>=1.21.1\n",
      "  Using cached urllib3-1.26.13-py2.py3-none-any.whl (140 kB)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\daso\\anaconda3\\envs\\funchatbot\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Collecting charset-normalizer<3,>=2\n",
      "  Using cached charset_normalizer-2.1.1-py3-none-any.whl (39 kB)\n",
      "Installing collected packages: tokenizers, urllib3, tqdm, regex, pyyaml, numpy, filelock, charset-normalizer, requests, huggingface-hub, transformers\n",
      "Successfully installed charset-normalizer-2.1.1 filelock-3.9.0 huggingface-hub-0.11.1 numpy-1.24.1 pyyaml-6.0 regex-2022.10.31 requests-2.28.1 tokenizers-0.13.2 tqdm-4.64.1 transformers-4.25.1 urllib3-1.26.13\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da026d61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ipywidgets\n",
      "  Downloading ipywidgets-8.0.4-py3-none-any.whl (137 kB)\n",
      "     -------------------------------------- 137.8/137.8 kB 1.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: ipykernel>=4.5.1 in c:\\users\\daso\\anaconda3\\envs\\funchatbot\\lib\\site-packages (from ipywidgets) (6.19.2)\n",
      "Collecting jupyterlab-widgets~=3.0\n",
      "  Downloading jupyterlab_widgets-3.0.5-py3-none-any.whl (384 kB)\n",
      "     -------------------------------------- 384.3/384.3 kB 3.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: ipython>=6.1.0 in c:\\users\\daso\\anaconda3\\envs\\funchatbot\\lib\\site-packages (from ipywidgets) (8.7.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in c:\\users\\daso\\anaconda3\\envs\\funchatbot\\lib\\site-packages (from ipywidgets) (5.7.1)\n",
      "Collecting widgetsnbextension~=4.0\n",
      "  Downloading widgetsnbextension-4.0.5-py3-none-any.whl (2.0 MB)\n",
      "     ---------------------------------------- 2.0/2.0 MB 5.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: comm>=0.1.1 in c:\\users\\daso\\anaconda3\\envs\\funchatbot\\lib\\site-packages (from ipykernel>=4.5.1->ipywidgets) (0.1.2)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in c:\\users\\daso\\anaconda3\\envs\\funchatbot\\lib\\site-packages (from ipykernel>=4.5.1->ipywidgets) (0.1.6)\n",
      "Requirement already satisfied: pyzmq>=17 in c:\\users\\daso\\anaconda3\\envs\\funchatbot\\lib\\site-packages (from ipykernel>=4.5.1->ipywidgets) (23.2.0)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in c:\\users\\daso\\anaconda3\\envs\\funchatbot\\lib\\site-packages (from ipykernel>=4.5.1->ipywidgets) (7.4.8)\n",
      "Requirement already satisfied: psutil in c:\\users\\daso\\anaconda3\\envs\\funchatbot\\lib\\site-packages (from ipykernel>=4.5.1->ipywidgets) (5.9.0)\n",
      "Requirement already satisfied: debugpy>=1.0 in c:\\users\\daso\\anaconda3\\envs\\funchatbot\\lib\\site-packages (from ipykernel>=4.5.1->ipywidgets) (1.5.1)\n",
      "Requirement already satisfied: tornado>=6.1 in c:\\users\\daso\\anaconda3\\envs\\funchatbot\\lib\\site-packages (from ipykernel>=4.5.1->ipywidgets) (6.2)\n",
      "Requirement already satisfied: nest-asyncio in c:\\users\\daso\\anaconda3\\envs\\funchatbot\\lib\\site-packages (from ipykernel>=4.5.1->ipywidgets) (1.5.6)\n",
      "Requirement already satisfied: packaging in c:\\users\\daso\\anaconda3\\envs\\funchatbot\\lib\\site-packages (from ipykernel>=4.5.1->ipywidgets) (22.0)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\daso\\anaconda3\\envs\\funchatbot\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (2.11.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\daso\\anaconda3\\envs\\funchatbot\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.4.6)\n",
      "Requirement already satisfied: pickleshare in c:\\users\\daso\\anaconda3\\envs\\funchatbot\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.7.5)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\daso\\anaconda3\\envs\\funchatbot\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.18.1)\n",
      "Requirement already satisfied: decorator in c:\\users\\daso\\anaconda3\\envs\\funchatbot\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: backcall in c:\\users\\daso\\anaconda3\\envs\\funchatbot\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: stack-data in c:\\users\\daso\\anaconda3\\envs\\funchatbot\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.11 in c:\\users\\daso\\anaconda3\\envs\\funchatbot\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (3.0.36)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in c:\\users\\daso\\anaconda3\\envs\\funchatbot\\lib\\site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: entrypoints in c:\\users\\daso\\anaconda3\\envs\\funchatbot\\lib\\site-packages (from jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets) (0.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\daso\\anaconda3\\envs\\funchatbot\\lib\\site-packages (from jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets) (2.8.2)\n",
      "Requirement already satisfied: jupyter-core>=4.9.2 in c:\\users\\daso\\anaconda3\\envs\\funchatbot\\lib\\site-packages (from jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\daso\\anaconda3\\envs\\funchatbot\\lib\\site-packages (from prompt-toolkit<3.1.0,>=3.0.11->ipython>=6.1.0->ipywidgets) (0.2.5)\n",
      "Requirement already satisfied: executing in c:\\users\\daso\\anaconda3\\envs\\funchatbot\\lib\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: asttokens in c:\\users\\daso\\anaconda3\\envs\\funchatbot\\lib\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.0.5)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\daso\\anaconda3\\envs\\funchatbot\\lib\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: pywin32>=1.0 in c:\\users\\daso\\anaconda3\\envs\\funchatbot\\lib\\site-packages (from jupyter-core>=4.9.2->jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets) (305.1)\n",
      "Requirement already satisfied: platformdirs>=2.5 in c:\\users\\daso\\anaconda3\\envs\\funchatbot\\lib\\site-packages (from jupyter-core>=4.9.2->jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets) (2.5.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\daso\\anaconda3\\envs\\funchatbot\\lib\\site-packages (from python-dateutil>=2.8.2->jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets) (1.16.0)\n",
      "Installing collected packages: widgetsnbextension, jupyterlab-widgets, ipywidgets\n",
      "Successfully installed ipywidgets-8.0.4 jupyterlab-widgets-3.0.5 widgetsnbextension-4.0.5\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install ipywidgets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81acd179",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b6fb774",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 571/571 [00:00<00:00, 95.2kB/s]\n",
      "Downloading: 100%|██████████| 1.34G/1.34G [04:26<00:00, 5.05MB/s]   \n",
      "Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-large-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Download the pre-trained BERT model (for example, 'bert-base-uncased')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForSequenceClassification.from_pretrained('bert-large-uncased')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6ff2fb55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SequenceClassifierOutput(loss=None, logits=tensor([[-0.2954, -0.5367]]), hidden_states=None, attentions=None)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Tokenize input\n",
    "text = \"[CLS] Who was Jim Henson ? [SEP]\"\n",
    "tokenized_text = tokenizer.tokenize(text)\n",
    "# Convert token to vocabulary indices\n",
    "indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "# Define the segments IDs\n",
    "segments_ids = [1] * len(tokenized_text)\n",
    "\n",
    "# Convert inputs to PyTorch tensors\n",
    "tokens_tensor = torch.tensor([indexed_tokens])\n",
    "segments_tensors = torch.tensor([segments_ids])\n",
    "\n",
    "#Predict the label for the input\n",
    "with torch.no_grad():\n",
    "    output = model(tokens_tensor, segments_tensors)\n",
    "    print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f7212b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = output[0]\n",
    "probs = torch.softmax(logits, dim=-1)\n",
    "predictions = probs.argmax(dim=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1d615e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-large-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot: Hello\n",
      "You: what is your name\n",
      "Chatbot: Hello\n",
      "You: hello\n",
      "Chatbot: Hello\n",
      "You: how can I help you\n",
      "Chatbot: Hi there! How can I help you?\n",
      "You: who are you\n",
      "Chatbot: Hello\n",
      "You: I want to create chatbot\n",
      "Chatbot: Hello\n",
      "You: I'm looking for to create chatbot\n",
      "Chatbot: Hello\n",
      "You: hi\n",
      "Chatbot: Hello\n",
      "You: fuck\n",
      "Chatbot: Hi there! How can I help you?\n",
      "You:     \"I'm looking to create a chatbot, Can you guide me on that?\",\n",
      "Chatbot: Hello\n",
      "You: how can yo help me\n",
      "Chatbot: Hello\n",
      "You: what you can do\n",
      "Chatbot: Hello\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "\n",
    "# Instantiate the tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForSequenceClassification.from_pretrained('bert-large-uncased')\n",
    "\n",
    "# Define some example conversation\n",
    "conversation = [\n",
    "    \"Hello\",\n",
    "    \"Hi there! How can I help you?\",\n",
    "    \"Can you tell me about yourself?\",\n",
    "    \"I am a pre-trained BERT model. I am trained to understand and generate natural language. What can I help you with?\",\n",
    "    \"I'm looking to create a chatbot, Can you guide me on that?\",\n",
    "    \"Sure! There are many ways to build a chatbot, depending on your specific requirements. One popular approach is to fine-tune a pre-trained language model like BERT on a dataset of conversational examples. Other techniques like natural language generation, dialogue management, and conversation optimization can also be used to improve the performance of your chatbot. Is there anything else I can help you with?\",\n",
    "    \"No that's all, Thanks!\"\n",
    "]\n",
    "\n",
    "# Start the conversation\n",
    "print(\"Chatbot: \" + conversation[0])\n",
    "\n",
    "while True:\n",
    "    # Get the user's input\n",
    "    user_input = input(\"You: \")\n",
    "    if user_input == conversation[-2]:\n",
    "        print(\"Chatbot: \" + conversation[-1])\n",
    "        break\n",
    "    # Tokenize the input\n",
    "    tokenized_text = tokenizer.tokenize(user_input)\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "    segments_ids = [1] * len(tokenized_text)\n",
    "\n",
    "    # Convert inputs to PyTorch tensors\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "    segments_tensors = torch.tensor([segments_ids])\n",
    "\n",
    "    #Predict the label for the input\n",
    "    with torch.no_grad():\n",
    "        output = model(tokens_tensor, segments_tensors)\n",
    "        logits = output[0]\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        predictions = probs.argmax(dim=-1)\n",
    "    print(\"Chatbot: \" + conversation[predictions[0]])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e907119",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
